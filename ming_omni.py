import torch
from transformers import (
    AutoProcessor,
    AutoModelForCausalLM,
    GenerationConfig,
)

# ======================
# åŸºæœ¬é…ç½®
# ======================
MODEL_ID = "inclusionAI/Ming-Lite-Omni"
DEVICE = "cuda"  # æ²¡ GPU å°±æ”¹æˆ "cpu"
DTYPE = torch.bfloat16  # GPU ä¸æ”¯æŒ bf16 å¯æ”¹æˆ torch.float16

IMAGE_PATH = "view.jpg"
AUDIO_PATH = "speechQA_sample.wav"


# ======================
# åŠ è½½æ¨¡å‹ä¸å¤„ç†å™¨
# ======================
def load_model_and_processor():
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=DTYPE,
        trust_remote_code=True,   # â­ å…³é”®ï¼šè‡ªåŠ¨åŠ è½½ modeling_bailingmm.py
        low_cpu_mem_usage=True,
    ).to(DEVICE)

    processor = AutoProcessor.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
    )

    return model, processor


# ======================
# é€šç”¨æ¨ç†å‡½æ•°
# ======================
@torch.inference_mode()
def run(messages, model, processor, use_whisper_encoder=False):
    # æ„é€  prompt
    text = processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
    )

    # è‡ªåŠ¨è§£æ image / audio
    image_inputs, video_inputs, audio_inputs = processor.process_vision_info(
        messages
    )

    audio_kwargs = {"use_whisper_encoder": True} if use_whisper_encoder else None

    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        audios=audio_inputs,
        return_tensors="pt",
        audio_kwargs=audio_kwargs,
    ).to(DEVICE)

    # å¯¹é½ dtypeï¼ˆå®˜æ–¹ç¤ºä¾‹å°±æ˜¯è¿™ä¹ˆåšçš„ï¼‰
    for k in ("pixel_values", "pixel_values_videos", "audio_feats"):
        if k in inputs:
            inputs[k] = inputs[k].to(DTYPE)

    gen_cfg = GenerationConfig(
        max_new_tokens=512,
        no_repeat_ngram_size=10,
    )

    outputs = model.generate(
        **inputs,
        generation_config=gen_cfg,
        eos_token_id=processor.gen_terminator,
        use_whisper_encoder=use_whisper_encoder,
    )

    # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
    gen_ids = outputs[0][inputs.input_ids.shape[1]:]
    return processor.decode(
        gen_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )


# ======================
# Image QA
# ======================
def image_qa(model, processor):
    messages = [
        {
            "role": "HUMAN",
            "content": [
                {"type": "image", "image": IMAGE_PATH},
                {"type": "text", "text": "What is shown in this image?"},
            ],
        }
    ]

    result = run(messages, model, processor)
    print("ğŸ–¼ Image QA Result:")
    print(result)


# ======================
# Audio QA
# ======================
def audio_qa(model, processor):
    messages = [
        {
            "role": "HUMAN",
            "content": [
                {"type": "text", "text": "Please summarize what is said in this audio."},
                {"type": "audio", "audio": AUDIO_PATH},
            ],
        }
    ]

    result = run(messages, model, processor)
    print("ğŸ”Š Audio QA Result:")
    print(result)


# ======================
# ASRï¼ˆå¯é€‰ï¼‰
# ======================
def asr(model, processor):
    messages = [
        {
            "role": "HUMAN",
            "content": [
                {"type": "audio", "audio": AUDIO_PATH},
                {"type": "text", "text": "Please transcribe this audio."},
            ],
        }
    ]

    result = run(
        messages,
        model,
        processor,
        use_whisper_encoder=True,  # â­ ASR å¿…é¡»å¼€
    )
    print("ğŸ“ ASR Result:")
    print(result)


# ======================
# ä¸»å…¥å£
# ======================
def main():
    model, processor = load_model_and_processor()

    print("\n===== Running Image QA =====")
    image_qa(model, processor)

    print("\n===== Running Audio QA =====")
    audio_qa(model, processor)

    # å¦‚éœ€ ASRï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Š
    # print("\n===== Running ASR =====")
    # asr(model, processor)


if __name__ == "__main__":
    main()
